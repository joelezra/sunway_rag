{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own RAG System: From Theory to Implementation\n",
    "\n",
    "Welcome to the hands-on demo where we'll:\n",
    "- Initialize Pinecone as our vector database.\n",
    "- Load a PDF document and split it into pages.\n",
    "- Generate 768-dimension embeddings using a SentenceTransformer model.\n",
    "- Upsert the embeddings into Pinecone.\n",
    "- Query the index and retrieve relevant text chunks.\n",
    "- Call a Gemini model endpoint (`gemini-2.0-flash`) to generate responses.\n",
    "- Launch a simple Streamlit chat interface that acts as a ChatGPT-like query interface.\n",
    "\n",
    "**Prerequisites:**  \n",
    "- Basic Python programming  \n",
    "- Familiarity with AI concepts  \n",
    "- Understanding of APIs & web services  \n",
    "- Bring your laptop!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"pinecone[grpc]\"\n",
    "%pip install PyPDF2\n",
    "%pip install sentence-transformers\n",
    "%pip install streamlit\n",
    "%pip install requests\n",
    "%pip install -q -U google-genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import subprocess\n",
    "from PyPDF2 import PdfReader\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use the class API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = '<add-api-key-here>'\n",
    "GEMINI_API_KEY = '<add-api-key-here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our Pinecone Client & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone with your API key and environment.\n",
    "# Replace 'YOUR_PINECONE_API_KEY' with your actual values.\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Define the index name and the dimension of our embedding model (768).\n",
    "if not pc.has_index(\"sunway-demo\"):\n",
    "    pc.create_index(\n",
    "        name=\"sunway-demo\",\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\", \n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    ) \n",
    "    \n",
    "# # Connect to the index\n",
    "# while not pc.describe_index(index_name).status['ready']:\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PDF using our PDF Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(filepath):\n",
    "    \"\"\"\n",
    "    Load a PDF file and return a list where each element is the text of a page.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        pages = [page.extract_text() for page in reader.pages]\n",
    "    return pages\n",
    "\n",
    "# Replace 'your_document.pdf' with the actual path to your PDF file.\n",
    "pdf_path = 'attention_sunway.pdf'\n",
    "pdf_pages = load_pdf(pdf_path)\n",
    "print(f\"Loaded {len(pdf_pages)} pages from the PDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the embeddings for the RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model.\n",
    "# We're using a model that produces 768-dimension embeddings. \n",
    "# 'all-mpnet-base-v2' is one such model; you can choose another if desired.\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "def generate_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate a 768-dimension embedding for the given text.\n",
    "    \"\"\"\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# Generate embeddings for each page of the PDF.\n",
    "embeddings = [generate_embedding(page) for page in pdf_pages]\n",
    "print(\"Generated embeddings for all pages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the embeddings for the Vector DB & Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the vectors for upserting. Each vector is assigned a unique ID and metadata.\n",
    "vectors = [\n",
    "    {\n",
    "        'id': f'page_{i}',\n",
    "        'values': embedding,\n",
    "        'metadata': {'page_number': i, 'text': pdf_pages[i]}\n",
    "    }\n",
    "    for i, embedding in enumerate(embeddings)\n",
    "]\n",
    "\n",
    "index = pc.Index(\"sunway-demo\")\n",
    "\n",
    "# Upsert the vectors into the Pinecone index.\n",
    "# index.upsert(vectors)\n",
    "print(f\"Upserted {len(vectors)} vectors into the Pinecone index.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query our DB for with an example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone(query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Query the Pinecone index for the top_k most similar text chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    print(query_embedding)\n",
    "    result = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    return result\n",
    "\n",
    "# Example: Query the index (this is just a test query)\n",
    "test_query = \"What is the transformer architecture?\"\n",
    "results = query_pinecone(test_query)\n",
    "context = \"\"\n",
    "# print(results)\n",
    "print(\"Query Results:\")\n",
    "for match in results['matches']:\n",
    "    print(f\"Score: {match['score']}\\nText: {match['metadata']['text']}\\n\")\n",
    "    context += f\"Score: {match['score']}\\nText: {match['metadata']['text']}\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the retrieved info + Gemini to answer our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "def generate_response(prompt, context):\n",
    "    \"\"\"\n",
    "    Call the Gemini model to generate a response, using the context from Pinecone.\n",
    "    \"\"\"\n",
    "    # Replace 'YOUR_API_KEY' with your actual Google AI API key.\n",
    "\n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\", contents=f\"Context: {context}\\\\n\\\\nPrompt: {prompt}\"\n",
    "    )\n",
    "    return response.text\n",
    "    \n",
    "# Test the generation function with sample context (this may not work until you set up the endpoint)\n",
    "# sample_context = \" \".join([match['metadata']['text'] for match in results['results'][0]['matches']])\n",
    "pprint.pprint(generate_response(test_query, context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, put this all in a Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile chat_app.py\n",
    "\n",
    "STREAMLIT_PINECONE_API_KEY = '<copy-paste-the-pinecone-api-key-here>'\n",
    "STREAMLIT_GEMINI_API_KEY = '<copy-paste-the-gemini-api-key-here>'\n",
    "STREAMLIT_INDEX_NAME = 'sunway-demo'\n",
    "\n",
    "import streamlit as st\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from google import genai\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(page_title=\"RAG Chatbot Demo\", layout=\"wide\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=STREAMLIT_PINECONE_API_KEY)\n",
    "index = pc.Index(STREAMLIT_INDEX_NAME)\n",
    "\n",
    "# Initialize Google Genai client\n",
    "client = genai.Client(api_key=STREAMLIT_GEMINI_API_KEY)\n",
    "\n",
    "# Load the embedding model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    return SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embeddings for text using SentenceTransformer\"\"\"\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "def query_pinecone(query_text, top_k=5):\n",
    "    \"\"\"Query Pinecone index for similar documents\"\"\"\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    return results\n",
    "\n",
    "def generate_response(prompt, context):\n",
    "    \"\"\"Generate response using Google's Gemini model\"\"\"\n",
    "    # client = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n",
    "    response = client.models.generate_content(contents = f\"Context: {context}\\n\\nPrompt: {prompt}\", model = \"gemini-2.0-flash\")\n",
    "    return response.text\n",
    "\n",
    "# Initialize session state for chat history\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat header\n",
    "st.title(\"📚 RAG Chatbot Demo\")\n",
    "st.markdown(\"Ask questions about the Transformer paper!\")\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Get user input\n",
    "if prompt := st.chat_input(\"What would you like to know about the Transformer architecture?\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Display assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        \n",
    "        # Show a spinner while processing\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            # Retrieve similar chunks from Pinecone\n",
    "            results = query_pinecone(prompt)\n",
    "            \n",
    "            # Format the context from retrieved documents\n",
    "            context = \"\"\n",
    "            for match in results['matches']:\n",
    "                context += f\"{match['metadata']['text']}\\n\\n\"\n",
    "            \n",
    "            # Generate a response using the Gemini model\n",
    "            response_text = generate_response(prompt, context)\n",
    "            \n",
    "            # Display the response\n",
    "            message_placeholder.markdown(response_text)\n",
    "    \n",
    "    # Add assistant response to chat history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "# Add info about the system\n",
    "with st.sidebar:\n",
    "    st.title(\"About\")\n",
    "    st.markdown(\"\"\"\n",
    "    This is a RAG (Retrieval-Augmented Generation) chatbot demo that:\n",
    "    \n",
    "    1. Takes your question\n",
    "    2. Finds relevant passages from the Transformer paper\n",
    "    3. Uses Google's Gemini model to generate a response based on the retrieved context\n",
    "    \n",
    "    The system uses:\n",
    "    - Pinecone for vector storage\n",
    "    - SentenceTransformer for embeddings\n",
    "    - Google Gemini for text generation\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Streamlit app in a new process.\n",
    "# This will open the chat interface in your default web browser.\n",
    "import subprocess\n",
    "subprocess.Popen([\"streamlit\", \"run\", \"chat_app.py\"])\n",
    "print(\"Streamlit chat interface launched.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "- Initialized Pinecone and created a vector index.\n",
    "- Loaded and processed a PDF document (Attention Is All You Need) into text pages.\n",
    "- Generated 768-dimensional embeddings for each page using a SentenceTransformer.\n",
    "- Uploaded these embeddings to Pinecone.\n",
    "- Defined functions to query the index and call a Gemini model endpoint.\n",
    "- Created and launched a simple Streamlit chat interface to interact with our RAG system.\n",
    "\n",
    "**Next Steps:**\n",
    "- Replace the API keys and Index name with your credentials.\n",
    "- Test and fine-tune the system with your specific PDF documents and queries.\n",
    "- Explore further enhancements to the chat interface and document processing!\n",
    "\n",
    "Happy coding and enjoy the workshop!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
